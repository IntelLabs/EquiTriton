{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd832b52-7df1-4d42-a2ea-a7139df2196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Callable, Any\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim import AdamW\n",
    "import e3nn\n",
    "from e3nn import o3\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.data import Data as PyGGraph\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_cluster import radius_graph\n",
    "import pytorch_lightning as pl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from equitriton.sph_harm.direct import triton_spherical_harmonic\n",
    "from equitriton.utils import spherical_harmonics_irreps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e68d7b9-c046-492c-8380-b801d6a0f209",
   "metadata": {},
   "source": [
    "# Baseline model development\n",
    "\n",
    "This notebook was used to develop the simple graph convolution model used in the paper _Deconstructing equivariant molecular representations in molecular systems_ at the AI4Mat workshop at NeurIPS 2024.\n",
    "\n",
    "If you are looking to use this architecture in your own testing, please look at the `equitriton.model.blocks` module for the \"production ready\" version (i.e. please don't copy-paste this code unless you're looking to modify things!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0307714-1036-4110-8b53-0c50062ee913",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = torch.manual_seed(215162)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f96b13-929a-4b2e-a4d9-991194fd98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module for embedding atomic numbers into dense vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_atoms : int\n",
    "        The number of distinct atomic types in the dataset.\n",
    "    atom_dim : int\n",
    "        The dimensionality of the embedded atomic vectors.\n",
    "\n",
    "    Example\n",
    "    --------\n",
    "    >>> # Create an instance of the AtomEmbedding module\n",
    "    >>> atom_embedding = AtomEmbedding(num_atoms=10, atom_dim=128)\n",
    "\n",
    "    >>> # Embed a batch of atomic numbers\n",
    "    >>> embedded_vectors = atom_embedding(torch.tensor([1, 2, 3, 4]))\n",
    "\n",
    "    >>> print(embedded_vectors.shape)  # Output: torch.Size([4, 128])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_atoms: int, atom_dim: int):\n",
    "        \"\"\"\n",
    "        Initializes the AtomEmbedding module.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_atoms : int\n",
    "            The number of distinct atomic types in the dataset.\n",
    "        atom_dim : int\n",
    "            The dimensionality of the embedded atomic vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_atoms, atom_dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, atomic_numbers: torch.LongTensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embeds a batch of atomic numbers into a tensor of embedded vectors.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        atomic_numbers : torch.LongTensor\n",
    "            A tensor of shape (batch_size,) containing the atomic numbers to embed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A tensor of shape (batch_size, atom_dim) containing the embedded atomic vectors.\n",
    "        \"\"\"\n",
    "        return self.embedding(atomic_numbers)\n",
    "\n",
    "\n",
    "class EdgeEmbedding(nn.Module):\n",
    "    def __init__(self, num_basis: int, radius_cutoff: float = 6.0, **kwargs):\n",
    "        \"\"\"\n",
    "        This module embeds edges in a graph with an EdgeEmbedding object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_basis : int, optional\n",
    "            The number of basis functions. Defaults to 1.\n",
    "        radius_cutoff : float, optional\n",
    "            The maximum radius up to which basis functions are defined. Defaults to 6.0.\n",
    "\n",
    "        Optional kwargs\n",
    "        ---------------\n",
    "        basis : str, optional\n",
    "            The type of basis function to use. Defaults to 'bessel'.\n",
    "        start : float, optional\n",
    "            The starting point in the distance grid used in the radial basis.\n",
    "        cutoff : bool, optional\n",
    "            Whether or not to apply a cutoff to the basis functions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A tensor representing the embedding of edges with shape (num_edges, num_basis).\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> # Define an instance of EdgeEmbedding with 4 basis functions and a radius cutoff of 10.\n",
    "        >>> embedder = EdgeEmbedding(num_basis=4, radius_cutoff=10.0)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        kwargs.setdefault(\"basis\", \"bessel\")\n",
    "        kwargs.setdefault(\"start\", 0.0)\n",
    "        kwargs.setdefault(\"cutoff\", True)\n",
    "        self.num_basis = num_basis\n",
    "        self.radius_cutoff = radius_cutoff\n",
    "        self.basis_kwargs = kwargs\n",
    "\n",
    "    def forward(self, distances: torch.Tensor) -> torch.Tensor:\n",
    "        basis_funcs = e3nn.math.soft_one_hot_linspace(\n",
    "            distances,\n",
    "            number=self.num_basis,\n",
    "            end=self.radius_cutoff,\n",
    "            **self.basis_kwargs,\n",
    "        )\n",
    "        return basis_funcs * self.num_basis**0.5\n",
    "\n",
    "\n",
    "class SphericalHarmonicEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        l_values: list[int],\n",
    "        normalize: bool = True,\n",
    "        normalization: Literal[\"norm\", \"integral\", \"component\"] = \"integral\",\n",
    "        use_e3nn: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Projects cartesian positions onto spherical harmonic functions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.l_values = list(sorted(l_values))\n",
    "        self.irreps = spherical_harmonics_irreps(self.l_values, num_feat=1)\n",
    "        self.normalize = normalize\n",
    "        self.normalization = normalization\n",
    "        self.use_e3nn = use_e3nn\n",
    "\n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.use_e3nn:\n",
    "            if self.normalize:\n",
    "                coords = torch.nn.functional.normalize(coords, dim=-1)\n",
    "            outputs = [triton_spherical_harmonic(l, coords) for l in self.l_values]\n",
    "            outputs = torch.cat(outputs, dim=-1)\n",
    "            if self.normalization == \"integral\":\n",
    "                outputs /= (4.0 * torch.pi) ** 0.5\n",
    "            return outputs\n",
    "        else:\n",
    "            return o3.spherical_harmonics(\n",
    "                self.irreps, coords, self.normalize, self.normalization\n",
    "            )\n",
    "\n",
    "\n",
    "class InteractionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        atomic_dim: int | o3.Irreps,\n",
    "        l_values: int,\n",
    "        edge_dim: int,\n",
    "        hidden_dim: int,\n",
    "        radius_cutoff: float,\n",
    "        degree_norm: float,\n",
    "        edge_kwargs: dict[str, Any] = {},\n",
    "        sph_harm_kwargs: dict[str, Any] = {},\n",
    "        activation: Callable = nn.functional.silu,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A module that combines radial basis with spherical harmonics to\n",
    "        describe molecular interactions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        atomic_dim : int | o3.Irreps\n",
    "            Dimension of the atomic features. If int, it is treated as a\n",
    "            single irreducible representation.\n",
    "        l_values : int\n",
    "            Values of the spherical harmonic order.\n",
    "        edge_dim : int\n",
    "            Dimension of the edge features.\n",
    "        hidden_dim : int\n",
    "            Hidden dimension for the fully connected network.\n",
    "        radius_cutoff : float\n",
    "            Cutoff radius for the radial basis.\n",
    "        degree_norm : float\n",
    "            Normalization factor for the degree of the graph.\n",
    "        edge_kwargs : dict[str, Any], optional\n",
    "            Keyword arguments for the EdgeEmbedding module. Defaults to {}.\n",
    "        sph_harm_kwargs : dict[str, Any], optional\n",
    "            Keyword arguments for the SphericalHarmonicEmbedding module.\n",
    "            Defaults to {}.\n",
    "        activation : Callable, optional\n",
    "            Activation function for the fully connected network. Defaults to\n",
    "            nn.functional.silu.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The `degree_norm` attribute is set as a property and effectively\n",
    "        represents the average number of neighbors in other models.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> block = InteractionBlock(atomic_dim=8, l_values=[0, 1],\n",
    "            edge_dim=16, hidden_dim=32)\n",
    "        >>> block.sph_irreps\n",
    "        ['1x0e', '2x0e']\n",
    "        \"\"\"\n",
    "        sph_harm_kwargs.setdefault(\"use_e3nn\", False)\n",
    "\n",
    "        super().__init__()\n",
    "        # this is effectively the average number of neighbors in other models\n",
    "        self.degree_norm = degree_norm\n",
    "        # treat atom features as invariant\n",
    "        if isinstance(atomic_dim, int):\n",
    "            atomic_irreps = f\"{atomic_dim}x0e\"\n",
    "        else:\n",
    "            atomic_irreps = atomic_dim\n",
    "        self.atomic_irreps = atomic_irreps\n",
    "        self.l_values = list(sorted(l_values))\n",
    "        # these two attributes are similar but different: the former is used for describing\n",
    "        # the basis itself, and the latter is for actually specifying the weights\n",
    "        self.sph_irreps = spherical_harmonics_irreps(self.l_values, num_feat=1)\n",
    "        self.output_irreps = spherical_harmonics_irreps(\n",
    "            self.l_values, num_feat=hidden_dim\n",
    "        )\n",
    "        # tensor product is the final bit the combines the radial basis with the spherical\n",
    "        # harmonics\n",
    "        self.tensor_product = o3.FullyConnectedTensorProduct(\n",
    "            self.atomic_irreps,\n",
    "            self.sph_irreps,\n",
    "            self.output_irreps,\n",
    "            shared_weights=False,\n",
    "        )\n",
    "        self.edge_basis = EdgeEmbedding(edge_dim, radius_cutoff, **edge_kwargs)\n",
    "        self.spherical_harmonics = SphericalHarmonicEmbedding(\n",
    "            l_values, **sph_harm_kwargs\n",
    "        )\n",
    "        self.fc = e3nn.nn.FullyConnectedNet(\n",
    "            [edge_dim, hidden_dim, self.tensor_product.weight_numel], activation\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_projections(self) -> int:\n",
    "        \"\"\"Returns the expected number of projections.\"\"\"\n",
    "        return sum([2 * l + 1 for l in self.l_values])\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        \"\"\"Returns the dimensionality of the output.\"\"\"\n",
    "        return self.output_irreps.dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atomic_features: torch.Tensor,\n",
    "        coords: torch.Tensor,\n",
    "        edge_index: torch.LongTensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        High-level description:\n",
    "\n",
    "        1. Project cartesian coordinates onto spherical harmonic basis\n",
    "        2. Project interatomic distances onto radial (bessel) basis\n",
    "        3. Transform radial basis functions with learnable weights\n",
    "        4. Compute tensor product between scalar atom features and spherical harmonic basis\n",
    "        5. Update node features\n",
    "        \"\"\"\n",
    "        edge_dist = coords[edge_index[0]] - coords[edge_index[1]]\n",
    "        sph_harm = self.spherical_harmonics(edge_dist)\n",
    "        # calculate atomic distances, embed, and transform them\n",
    "        edge_basis = self.edge_basis(edge_dist.norm(dim=-1))\n",
    "        edge_z = self.fc(edge_basis)\n",
    "        # compute tensor product\n",
    "        messages = self.tensor_product(atomic_features[edge_index[0]], sph_harm, edge_z)\n",
    "        # update node features\n",
    "        hidden_feats = (\n",
    "            scatter(messages, edge_index[1], dim=0, dim_size=atomic_features.size(0))\n",
    "            / self.degree_norm\n",
    "        )\n",
    "        return hidden_feats\n",
    "\n",
    "\n",
    "class ScalarReadoutLayer(nn.Module):\n",
    "    def __init__(self, hidden_irreps: o3.Irreps, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_irreps = hidden_irreps\n",
    "        self.output_irreps = o3.Irreps(f\"{output_dim}x0e\")\n",
    "        self.output_layer = o3.Linear(\n",
    "            irreps_in=hidden_irreps, irreps_out=self.output_irreps\n",
    "        )\n",
    "\n",
    "    def forward(self, node_feats: torch.Tensor) -> torch.Tensor:\n",
    "        return self.output_layer(node_feats)\n",
    "\n",
    "\n",
    "class EquiTritonModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_atom_dim: int,\n",
    "        num_layers: int,\n",
    "        output_dim: int,\n",
    "        l_values: int,\n",
    "        edge_dim: int,\n",
    "        hidden_dim: int,\n",
    "        radius_cutoff: float,\n",
    "        degree_norm: float,\n",
    "        edge_kwargs: dict[str, Any] = {},\n",
    "        sph_harm_kwargs: dict[str, Any] = {},\n",
    "        activation: Callable = nn.functional.silu,\n",
    "        num_atoms: int = 100,\n",
    "        skip_connections: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        End-to-end simple model that uses the EquiTriton kernels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_atom_dim : int\n",
    "            The dimensionality of the atomic embeddings.\n",
    "        num_layers : int\n",
    "            The number of convolutional layers in the model.\n",
    "        output_dim : int\n",
    "            The dimensionality of the graph-level scalar features.\n",
    "        l_values : int\n",
    "            The maximum value of the L indices for spherical harmonics.\n",
    "        edge_dim : int\n",
    "            The dimensionality of the edge features.\n",
    "        hidden_dim : int\n",
    "            The hidden dimensionality of the interaction blocks.\n",
    "        radius_cutoff : float\n",
    "            The cutoff distance for the radial basis functions.\n",
    "        degree_norm : float\n",
    "            The normalization factor for the degree of each node.\n",
    "        edge_kwargs : dict, optional\n",
    "            Additional keyword arguments for the edge embedding layer. Defaults to {}.\n",
    "        sph_harm_kwargs : dict, optional\n",
    "            Additional keyword arguments for the spherical harmonics layer. Defaults to {}.\n",
    "        activation : Callable, optional\n",
    "            The activation function used in the model. Defaults to nn.functional.silu.\n",
    "        num_atoms : int, optional\n",
    "            The number of atoms in each graph. Defaults to 100.\n",
    "        skip_connections : bool, optional\n",
    "            Whether to use skip connections between layers. Defaults to True.\n",
    "        \"\"\"\n",
    "        sph_harm_kwargs.setdefault(\"use_e3nn\", False)\n",
    "\n",
    "        super().__init__()\n",
    "        self.atomic_embedding = AtomEmbedding(num_atoms, initial_atom_dim)\n",
    "        self.initial_layer = InteractionBlock(\n",
    "            initial_atom_dim,\n",
    "            l_values,\n",
    "            edge_dim,\n",
    "            hidden_dim,\n",
    "            radius_cutoff,\n",
    "            degree_norm,\n",
    "            edge_kwargs,\n",
    "            sph_harm_kwargs,\n",
    "            activation,\n",
    "        )\n",
    "        self.conv_layers = nn.ModuleDict()\n",
    "        for layer_index in range(num_layers + 1):\n",
    "            self.conv_layers[f\"conv_{layer_index}\"] = InteractionBlock(\n",
    "                self.initial_layer.output_irreps,\n",
    "                l_values,\n",
    "                edge_dim,\n",
    "                hidden_dim,\n",
    "                radius_cutoff,\n",
    "                degree_norm,\n",
    "                edge_kwargs,\n",
    "                sph_harm_kwargs,\n",
    "                activation,\n",
    "            )\n",
    "        self.scalar_readout = ScalarReadoutLayer(\n",
    "            self.initial_layer.output_irreps, output_dim\n",
    "        )\n",
    "        self.skip_connections = skip_connections\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def visualize(self, **kwargs):\n",
    "        num_plots = len(self.conv_layers) + 1\n",
    "        fig, axarray = plt.subplots(num_plots, 1, figsize=(3, 12))\n",
    "        # make indexing easier\n",
    "        axarray = axarray.flatten()\n",
    "\n",
    "        self.initial_layer.tensor_product.visualize(ax=axarray[0], **kwargs)\n",
    "        axarray[0].set_title(\"Input layer\", loc=\"right\")\n",
    "        index = 1\n",
    "        for layer_name, layer in self.conv_layers.items():\n",
    "            ax = axarray[index]\n",
    "            layer.tensor_product.visualize(ax=ax, **kwargs)\n",
    "            ax.set_title(layer_name, loc=\"right\")\n",
    "            index += 1\n",
    "        fig.tight_layout()\n",
    "        return fig, axarray\n",
    "\n",
    "    def forward(self, graph: PyGGraph) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # determine if the graph is batched or not\n",
    "        is_batched = hasattr(graph, \"ptr\")\n",
    "        # get atom embeddings\n",
    "        atom_z = self.atomic_embedding(graph.z)  # [nodes, initial_atom_dim]\n",
    "        # first message passing step\n",
    "        z = self.initial_layer(atom_z, graph.pos, graph.edge_index)\n",
    "        outputs = {}\n",
    "        for layer_name, layer in self.conv_layers.items():\n",
    "            new_z = layer(z, graph.pos, graph.edge_index)\n",
    "            # add residual connections\n",
    "            if self.skip_connections and new_z.shape == z.shape:\n",
    "                new_z += z\n",
    "            z = new_z\n",
    "            outputs[layer_name] = z\n",
    "        # map final output as scalars\n",
    "        z = self.scalar_readout(z)\n",
    "        # latest node features are in z; we generate graph-level scalar features\n",
    "        # by doing a scatter add\n",
    "        if is_batched:\n",
    "            graph_z = scatter(z, graph.batch, dim=0, dim_size=graph.batch_size)\n",
    "        else:\n",
    "            # for a single graph, just sum up the node features\n",
    "            graph_z = z.sum(dim=0, keepdims=True)\n",
    "        return graph_z, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e568b72-3990-4652-8a32-db375c65a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fake_graph(\n",
    "    num_nodes: int,\n",
    "    num_edges: int,\n",
    "    max_radius: float = 1.5,\n",
    "    coord_scale: float = 1.0,\n",
    "    max_atomic_number: int = 100,\n",
    "    device=\"xpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a fake graph with the specified number of nodes and edges.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_nodes : int\n",
    "        The number of nodes in the graph.\n",
    "    num_edges : int\n",
    "        The number of edges in the graph.\n",
    "    max_radius : float, optional\n",
    "        The maximum radius for node connections. Defaults to 1.5.\n",
    "    coord_scale : float, optional\n",
    "        The scaling factor for node coordinates. Defaults to 1.0.\n",
    "    max_atomic_number : int, optional\n",
    "        The maximum atomic number for nodes. Defaults to 100.\n",
    "    device : str or torch.device, optional\n",
    "        The device to use for computations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "        A tuple containing:\n",
    "        - coords (torch.Tensor): Node coordinates with shape ``(num_nodes, 3)``.\n",
    "        - edge_index (torch.Tensor): Edge indices with shape ``(2, num_edges)``.\n",
    "        - atomic_numbers (torch.Tensor): Atomic numbers for nodes with shape ``(num_nodes,)``.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> coords, edge_index, atomic_numbers = make_fake_graph(10, 20)\n",
    "    >>> print(coords.shape)  # (10, 3)\n",
    "    >>> print(edge_index.shape)  # (2, 20)\n",
    "    >>> print(atomic_numbers.shape)  # (10,)\n",
    "    \"\"\"\n",
    "    coords = torch.rand(num_nodes, 3, device=device) * coord_scale\n",
    "    edge_src, edge_dst = radius_graph(\n",
    "        coords, max_radius, max_num_neighbors=num_nodes - 1\n",
    "    )\n",
    "    edge_index = torch.vstack([edge_src, edge_dst]).to(device)\n",
    "    atomic_numbers = torch.randint(\n",
    "        0, max_atomic_number, size=(num_nodes,), device=device\n",
    "    )\n",
    "    return coords, edge_index, atomic_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8671f0e8-0da8-4250-9d02-fb30ecb977fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_embedder = EdgeEmbedding(num_basis=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4971d48-3574-44d0-b2fb-27306bc3aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords, edge_index, atomic_numbers = make_fake_graph(\n",
    "    16,\n",
    "    12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f14f94-51ca-4e5c-ad63-ca830404eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords = torch.ones_like(coords, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f6e836-f4e0-48cf-981d-4419dda0159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_embedder = AtomEmbedding(100, 64).to(\"cuda\")\n",
    "atom_z = atom_embedder(atomic_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46778ea6-92f2-4d7a-9bd8-aa58013cd1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 240])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c82e80f-c4bd-43fe-8846-dab203757992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "layer = InteractionBlock(\n",
    "    64,\n",
    "    [\n",
    "        0,\n",
    "        1,\n",
    "        2,\n",
    "    ],\n",
    "    10,\n",
    "    32,\n",
    "    radius_cutoff=6.0,\n",
    "    degree_norm=17**0.5,\n",
    "    sph_harm_kwargs={\"use_e3nn\": True},\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0913f1e-b000-4a3b-b723-84a92d16425d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "next_layer = InteractionBlock(\n",
    "    layer.output_irreps,\n",
    "    [\n",
    "        0,\n",
    "        1,\n",
    "        2,\n",
    "    ],\n",
    "    10,\n",
    "    32,\n",
    "    radius_cutoff=6.0,\n",
    "    degree_norm=17**0.5,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e893b896-f7b9-4b62-95a1-fe7fb45cf2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InteractionBlock(\n",
       "  (tensor_product): FullyConnectedTensorProduct(64x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 6144 paths | 6144 weights)\n",
       "  (edge_basis): EdgeEmbedding()\n",
       "  (spherical_harmonics): SphericalHarmonicEmbedding()\n",
       "  (fc): FullyConnectedNet[10, 32, 6144]\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34c3578d-0017-4350-99d2-373ad2e542cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Figure size 640x480 with 1 Axes>, <Axes: >)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer.tensor_product.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2785a28-4d2b-4cf0-972d-60df70595580",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = layer(atom_z, coords, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b80d729-b5e4-4cb5-802d-c27f2af32a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3004, -0.0939, -0.3610,  ...,  0.5229, -0.7874, -1.0619],\n",
       "        [-0.3039, -0.4817, -0.9658,  ...,  0.2480, -0.3691, -0.5077],\n",
       "        [-0.6431, -0.0786, -0.3443,  ...,  0.2488, -0.2363, -0.1527],\n",
       "        ...,\n",
       "        [-0.2051,  0.1712,  0.3823,  ...,  0.6626, -0.1357, -0.1973],\n",
       "        [ 0.0641,  0.1162, -1.2586,  ...,  0.2460,  0.7797,  0.2671],\n",
       "        [ 0.5735, -0.6425, -0.1460,  ..., -0.0062,  0.0575,  0.3319]],\n",
       "       device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dd937d-2581-4d09-954b-ac3ace509c33",
   "metadata": {},
   "source": [
    "## Equivariance check\n",
    "\n",
    "Uses `e3nn` tooling to generate the random rotation matrix, and output as a function of rotation permutation: rotating the coordinates before passing into the layer, and rotating the transformed embeddings with the same rotation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4ff86c7-e8e5-4560-83b9-fc1885fd8d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# [0, 1, 2] are necessary at the minimum, but all orders should work\n",
    "layer = InteractionBlock(\n",
    "    64,\n",
    "    [0, 1, 2, 3, 4, 5, 7],\n",
    "    10,\n",
    "    32,\n",
    "    radius_cutoff=6.0,\n",
    "    degree_norm=17**0.5,\n",
    "    sph_harm_kwargs={\"use_e3nn\": True},  # this can be toggled for comparison\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2633995-0e41-4bf2-8093-5f68a92242ae",
   "metadata": {},
   "source": [
    "### Rotation check\n",
    "\n",
    "This performs a random rotation to the coordinates, and we check the embeddings with and without the rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a3a0a02-8037-4784-9771-3abf8dad7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_matrix = o3.rand_matrix()\n",
    "\n",
    "# dims_in doesn't actually do anything in the case where the features are scalar\n",
    "dims_in = o3.Irreps(layer.atomic_irreps).D_from_matrix(rot_matrix).to(\"cuda\")\n",
    "# but dims_out actually does something, since the output features/embeddings need\n",
    "# to be rotated based on the same rotation matrix\n",
    "dims_out = layer.output_irreps.D_from_matrix(rot_matrix).to(\"cuda\")\n",
    "\n",
    "# rotate coordinates before passing into the layer\n",
    "rot_before = layer(atom_z @ dims_in.T, coords @ rot_matrix.T.to(\"cuda\"), edge_index)\n",
    "# rotate layer output by the same rotation matrix\n",
    "rot_after = layer(atom_z, coords, edge_index) @ dims_out.T\n",
    "\n",
    "assert torch.allclose(rot_before, rot_after, rtol=1e-7, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc060469-723a-4ece-9716-1b1fbc936806",
   "metadata": {},
   "source": [
    "### Rotation + translation\n",
    "\n",
    "If all atoms are shifted by a vector, the Bessel embedding should also be the same as it works solely on interatom distances, i.e. we do not need to shift the output embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fea72969-5f5a-410e-9a28-75346701498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_matrix = o3.rand_matrix()\n",
    "# shift all coordinates by the same amount in space\n",
    "trans_matrix = torch.randn(size=(1, 3), device=\"cuda\")\n",
    "\n",
    "# dims_in doesn't actually do anything in the case where the features are scalar\n",
    "dims_in = o3.Irreps(layer.atomic_irreps).D_from_matrix(rot_matrix).to(\"cuda\")\n",
    "dims_out = layer.output_irreps.D_from_matrix(rot_matrix).to(\"cuda\")\n",
    "\n",
    "# rotate and translate coordinates before passing into the layer\n",
    "rot_before = layer(\n",
    "    atom_z @ dims_in.T, coords @ rot_matrix.T.to(\"cuda\") + trans_matrix, edge_index\n",
    ")\n",
    "# rotate layer output by the same rotation matrix\n",
    "rot_after = layer(atom_z, coords, edge_index) @ dims_out.T\n",
    "\n",
    "assert torch.allclose(rot_before, rot_after, rtol=1e-7, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ad5de-1172-4bdf-83c9-92eda962a398",
   "metadata": {},
   "source": [
    "## Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e448746-66f8-484f-8971-1c89c8f75135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningQM9(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_path: str = \"./qm9_data\",\n",
    "        batch_size: int = 16,\n",
    "        train_frac: float = 0.8,\n",
    "        val_frac: float = 0.1,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom data module for QM9 dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        root_path : str, optional (default: \"./qm9_data\")\n",
    "            Path to the QM9 dataset.\n",
    "        batch_size : int, optional (default: 16)\n",
    "            Number of samples in each mini-batch.\n",
    "        train_frac : float, optional (default: 0.8)\n",
    "            Fraction of data used for training.\n",
    "        val_frac : float, optional (default: 0.1)\n",
    "            Fraction of data used for validation.\n",
    "        num_workers : int, optional (default: 0)\n",
    "            Number of worker processes to use for loading data.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> dm = LightningQM9(root_path=\"/path/to/qm9_data\", batch_size=32)\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        dataset : QM9\n",
    "            Loaded QM9 dataset.\n",
    "        hparams : dict\n",
    "            Hyperparameters of the data module.\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        setup(stage: str)\n",
    "            Setup data splits for training, validation and testing.\n",
    "        train_dataloader()\n",
    "            Returns a DataLoader instance for training data.\n",
    "        val_dataloader()\n",
    "            Returns a DataLoader instance for validation data.\n",
    "        test_dataloader()\n",
    "            Returns a DataLoader instance for testing data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset = QM9(root_path)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        hparams = self.hparams\n",
    "        num_samples = len(self.dataset)\n",
    "        num_train = int(num_samples * hparams[\"train_frac\"])\n",
    "        num_val = int(num_samples * hparams[\"val_frac\"])\n",
    "        num_test = ceil(\n",
    "            num_samples * (1 - (hparams[\"train_frac\"] + hparams[\"val_frac\"]))\n",
    "        )\n",
    "        # generate random splits\n",
    "        train_split, val_split, test_split = random_split(\n",
    "            self.dataset, lengths=[num_train, num_val, num_test]\n",
    "        )\n",
    "        self.splits = {\"train\": train_split, \"val\": val_split, \"test\": test_split}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.splits[\"train\"],\n",
    "            batch_size=self.hparams[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams[\"num_workers\"],\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.splits[\"val\"],\n",
    "            batch_size=self.hparams[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams[\"num_workers\"],\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.splits[\"test\"],\n",
    "            batch_size=self.hparams[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams[\"num_workers\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15f669-04ca-418b-8f22-81993eafbce0",
   "metadata": {},
   "source": [
    "## Loss and Lightning module\n",
    "\n",
    "Model trains optionally with a loss target that Nequip and MACE uses, which is the atom-weighted MSE. For now we're only using a single target, but can expand to use the full QM9 set of targets too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6affd332-fe3a-49dd-b0c2-e20fcb974a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomWeightedMSE(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the mean-squared-error between predicted and targets,\n",
    "    weighted by the number of atoms within each graph.\n",
    "\n",
    "    From matsciml\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        atoms_per_graph: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        if atoms_per_graph.size(0) != target.size(0):\n",
    "            raise RuntimeError(\n",
    "                \"Dimensions for atom-weighted loss do not match:\"\n",
    "                f\" expected atoms_per_graph to have {target.size(0)} elements; got {atoms_per_graph.size(0)}.\"\n",
    "                \"This loss is intended to be applied to scalar targets only.\"\n",
    "            )\n",
    "        # check to make sure we are broad casting correctly\n",
    "        if (input.ndim != target.ndim) and target.size(-1) == 1:\n",
    "            input.unsqueeze_(-1)\n",
    "        # for N-d targets, we might want to keep unsqueezing\n",
    "        while atoms_per_graph.ndim < target.ndim:\n",
    "            atoms_per_graph.unsqueeze_(-1)\n",
    "        # ensures that atoms_per_graph is type cast correctly\n",
    "        squared_error = ((input - target) / atoms_per_graph.to(input.dtype)) ** 2.0\n",
    "        return squared_error.mean()\n",
    "\n",
    "\n",
    "class EquiTritonLitModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class: type,\n",
    "        model_kwargs,\n",
    "        e_mean: float,\n",
    "        e_std: float,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 0.0,\n",
    "        atom_weighted_loss: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the EquiTritonLitModule clas.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_class : type\n",
    "            Th class of the model to be used.\n",
    "        model_kwargs : dict\n",
    "            Keyword argument for the model initialization.\n",
    "        e_mean : float\n",
    "            The mean of the energy values.\n",
    "        e_std : float\n",
    "            The standard deviation of the energy values.\n",
    "        lr : float, optional\n",
    "            The learning rate (default is 1e-3) for AdamW.\n",
    "        weight_decay : float, optional\n",
    "            Weight decay value (default is 0.0).\n",
    "        atom_weighted_loss : bool, optional\n",
    "            Whether to use atom-weighted loss or not (default is True).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model_class(**model_kwargs)\n",
    "        if atom_weighted_loss:\n",
    "            self.loss = AtomWeightedMSE()\n",
    "        else:\n",
    "            self.loss = nn.MSELoss()\n",
    "        self.output_head = nn.Linear(self.model.output_dim, 1)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams[\"lr\"],\n",
    "            weight_decay=self.hparams[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "    def step(self, graph: PyGGraph, stage: Literal[\"train\", \"test\", \"val\"]):\n",
    "        \"\"\"\n",
    "        Performs a single step of the training, validation or testing\n",
    "        process.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph : PyGGraph\n",
    "            The input graph.\n",
    "        stage : Literal[\"train\", \"test\", \"val\"]\n",
    "            The current stage (training, testing or validation).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            The calculated loss value.\n",
    "        \"\"\"\n",
    "        g_z, z = self.model(graph)\n",
    "        pred_energy = self.output_head(g_z)\n",
    "        target_energy = graph.y[:, 12].unsqueeze(-1)\n",
    "        norm_energy = (target_energy - self.hparams[\"e_mean\"]) / self.hparams[\"e_std\"]\n",
    "        if self.hparams[\"atom_weighted_loss\"]:\n",
    "            loss = self.loss(pred_energy, norm_energy, torch.diff(graph.ptr))\n",
    "        else:\n",
    "            loss = self.loss(pred_energy, norm_energy)\n",
    "        batch_size = getattr(graph, \"batch_size\", 1)\n",
    "        self.log(\n",
    "            f\"{stage}_loss\", loss, prog_bar=True, on_step=True, batch_size=batch_size\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss = self.step(batch, \"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        loss = self.step(batch, \"val\")\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        loss = self.step(batch, \"test\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d7ce968-f33c-46dd-88e3-8ad47480a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch_geometric/io/fs.py:215: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location)\n"
     ]
    }
   ],
   "source": [
    "dm = LightningQM9(\"./qm9_data/\", batch_size=64)\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "train_loader = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c89a710-1e68-485c-9325-0b06c7b52b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.cat([sample.y[:, 12] for sample in dm.dataset])\n",
    "e_mean = values.mean()\n",
    "e_std = values.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1738835c-76e4-4ef9-acf7-df3d5f20d449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-76.1160), tensor(10.3238))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_mean, e_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef3c9902-3673-4c88-a98e-1a4553a5990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lit_mod = EquiTritonLitModule(\n",
    "    EquiTritonModel,\n",
    "    model_kwargs={\n",
    "        \"initial_atom_dim\": 64,\n",
    "        \"num_layers\": 3,\n",
    "        \"output_dim\": 48,\n",
    "        \"l_values\": [0, 1, 2, 5, 6],\n",
    "        \"edge_dim\": 10,\n",
    "        \"hidden_dim\": 16,\n",
    "        \"radius_cutoff\": 6.0,\n",
    "        \"degree_norm\": 37.5**0.5,\n",
    "    },\n",
    "    e_mean=e_mean,\n",
    "    e_std=e_std,\n",
    "    atom_weighted_loss=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1104fe2c-ada6-448b-9e3d-cab1bc491c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=30, accelerator=\"gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e30a0671-efa3-403b-9d8b-cc2cb63bff20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type            | Params\n",
      "------------------------------------------------\n",
      "0 | model       | EquiTritonModel | 630 K \n",
      "1 | loss        | MSELoss         | 0     \n",
      "2 | output_head | Linear          | 49    \n",
      "------------------------------------------------\n",
      "630 K     Trainable params\n",
      "0         Non-trainable params\n",
      "630 K     Total params\n",
      "2.522     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592377c70fe5446c8474681ceb2ef7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_mod, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663a128-606c-4944-8877-73deafc30305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
