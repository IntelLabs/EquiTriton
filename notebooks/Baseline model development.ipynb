{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd832b52-7df1-4d42-a2ea-a7139df2196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Callable, Any\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim import AdamW\n",
    "import e3nn\n",
    "from e3nn import o3\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.data import Data as PyGGraph\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from equitriton.sph_harm.direct import triton_spherical_harmonic\n",
    "from equitriton.utils import spherical_harmonics_irreps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0307714-1036-4110-8b53-0c50062ee913",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = torch.manual_seed(215162)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79f96b13-929a-4b2e-a4d9-991194fd98ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomEmbedding(nn.Module):\n",
    "    def __init__(self, num_atoms: int, atom_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_atoms, atom_dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, atomic_numbers: torch.LongTensor) -> torch.Tensor:\n",
    "        return self.embedding(atomic_numbers)\n",
    "\n",
    "\n",
    "class EdgeEmbedding(nn.Module):\n",
    "    def __init__(self, num_basis: int, radius_cutoff: float = 6.0, **kwargs):\n",
    "        \"\"\"\n",
    "        This module embeds edges in a graph with an EdgeEmbedding object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_basis : int, optional\n",
    "            The number of basis functions. Defaults to 1.\n",
    "        radius_cutoff : float, optional\n",
    "            The maximum radius up to which basis functions are defined. Defaults to 6.0.\n",
    "\n",
    "        Optional kwargs\n",
    "        ---------------\n",
    "        basis : str, optional\n",
    "            The type of basis function to use. Defaults to 'bessel'.\n",
    "        start : float, optional\n",
    "            The starting point in the distance grid used in the radial basis.\n",
    "        cutoff : bool, optional\n",
    "            Whether or not to apply a cutoff to the basis functions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A tensor representing the embedding of edges with shape (num_edges, num_basis).\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> # Define an instance of EdgeEmbedding with 4 basis functions and a radius cutoff of 10.\n",
    "        >>> embedder = EdgeEmbedding(num_basis=4, radius_cutoff=10.0)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        kwargs.setdefault(\"basis\", \"bessel\")\n",
    "        kwargs.setdefault(\"start\", 0.0)\n",
    "        kwargs.setdefault(\"cutoff\", True)\n",
    "        self.num_basis = num_basis\n",
    "        self.radius_cutoff = radius_cutoff\n",
    "        self.basis_kwargs = kwargs\n",
    "\n",
    "    def forward(self, distances: torch.Tensor) -> torch.Tensor:\n",
    "        basis_funcs = e3nn.math.soft_one_hot_linspace(\n",
    "            distances,\n",
    "            number=self.num_basis,\n",
    "            end=self.radius_cutoff,\n",
    "            **self.basis_kwargs,\n",
    "        )\n",
    "        return basis_funcs * self.num_basis**0.5\n",
    "\n",
    "\n",
    "class SphericalHarmonicEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        l_values: list[int],\n",
    "        normalize: bool = True,\n",
    "        normalization: Literal[\"norm\", \"integral\", \"component\"] = \"integral\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Projects cartesian positions onto spherical harmonic functions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.l_values = list(sorted(l_values))\n",
    "        self.irreps = spherical_harmonics_irreps(self.l_values, num_feat=1)\n",
    "        self.normalize = normalize\n",
    "        self.normalization = normalization\n",
    "\n",
    "    def forward(self, coords: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = [triton_spherical_harmonic(l, coords) for l in self.l_values]\n",
    "        return torch.cat(outputs, dim=-1)\n",
    "\n",
    "\n",
    "class InteractionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        atomic_dim: int | o3.Irreps,\n",
    "        l_values: int,\n",
    "        edge_dim: int,\n",
    "        hidden_dim: int,\n",
    "        radius_cutoff: float,\n",
    "        degree_norm: float,\n",
    "        edge_kwargs: dict[str, Any] = {},\n",
    "        sph_harm_kwargs: dict[str, Any] = {},\n",
    "        activation: Callable = nn.functional.silu,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A module that combines radial basis with spherical harmonics to\n",
    "        describe molecular interactions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        atomic_dim : int | o3.Irreps\n",
    "            Dimension of the atomic features. If int, it is treated as a\n",
    "            single irreducible representation.\n",
    "        l_values : int\n",
    "            Values of the spherical harmonic order.\n",
    "        edge_dim : int\n",
    "            Dimension of the edge features.\n",
    "        hidden_dim : int\n",
    "            Hidden dimension for the fully connected network.\n",
    "        radius_cutoff : float\n",
    "            Cutoff radius for the radial basis.\n",
    "        degree_norm : float\n",
    "            Normalization factor for the degree of the graph.\n",
    "        edge_kwargs : dict[str, Any], optional\n",
    "            Keyword arguments for the EdgeEmbedding module. Defaults to {}.\n",
    "        sph_harm_kwargs : dict[str, Any], optional\n",
    "            Keyword arguments for the SphericalHarmonicEmbedding module.\n",
    "            Defaults to {}.\n",
    "        activation : Callable, optional\n",
    "            Activation function for the fully connected network. Defaults to\n",
    "            nn.functional.silu.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The `degree_norm` attribute is set as a property and effectively\n",
    "        represents the average number of neighbors in other models.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> block = InteractionBlock(atomic_dim=8, l_values=[0, 1],\n",
    "            edge_dim=16, hidden_dim=32)\n",
    "        >>> block.sph_irreps\n",
    "        ['1x0e', '2x0e']\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        # this is effectively the average number of neighbors in other models\n",
    "        self.degree_norm = degree_norm\n",
    "        # treat atom features as invariant\n",
    "        if isinstance(atomic_dim, int):\n",
    "            atomic_irreps = f\"{atomic_dim}x0e\"\n",
    "        else:\n",
    "            atomic_irreps = atomic_dim\n",
    "        self.atomic_irreps = atomic_irreps\n",
    "        self.l_values = list(sorted(l_values))\n",
    "        # these two attributes are similar but different: the former is used for describing\n",
    "        # the basis itself, and the latter is for actually specifying the weights\n",
    "        self.sph_irreps = spherical_harmonics_irreps(self.l_values, num_feat=1)\n",
    "        self.output_irreps = spherical_harmonics_irreps(\n",
    "            self.l_values, num_feat=hidden_dim\n",
    "        )\n",
    "        # tensor product is the final bit the combines the radial basis with the spherical\n",
    "        # harmonics\n",
    "        self.tensor_product = o3.FullyConnectedTensorProduct(\n",
    "            self.atomic_irreps,\n",
    "            self.sph_irreps,\n",
    "            self.output_irreps,\n",
    "            shared_weights=False,\n",
    "        )\n",
    "        self.edge_basis = EdgeEmbedding(edge_dim, radius_cutoff, **edge_kwargs)\n",
    "        self.spherical_harmonics = SphericalHarmonicEmbedding(\n",
    "            l_values, **sph_harm_kwargs\n",
    "        )\n",
    "        self.fc = e3nn.nn.FullyConnectedNet(\n",
    "            [edge_dim, hidden_dim, self.tensor_product.weight_numel], activation\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def num_projections(self) -> int:\n",
    "        \"\"\"Returns the expected number of projections.\"\"\"\n",
    "        return sum([2 * l + 1 for l in self.l_values])\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        \"\"\"Returns the dimensionality of the output.\"\"\"\n",
    "        return self.output_irreps.dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atomic_features: torch.Tensor,\n",
    "        coords: torch.Tensor,\n",
    "        edge_index: torch.LongTensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        High-level description:\n",
    "\n",
    "        1. Project cartesian coordinates onto spherical harmonic basis\n",
    "        2. Project interatomic distances onto radial (bessel) basis\n",
    "        3. Transform radial basis functions with learnable weights\n",
    "        4. Compute tensor product between scalar atom features and spherical harmonic basis\n",
    "        5. Update node features\n",
    "        \"\"\"\n",
    "        edge_dist = coords[edge_index[0]] - coords[edge_index[1]]\n",
    "        sph_harm = self.spherical_harmonics(edge_dist)\n",
    "        # calculate atomic distances, embed, and transform them\n",
    "        edge_basis = self.edge_basis(edge_dist.norm(dim=-1))\n",
    "        edge_z = self.fc(edge_basis)\n",
    "        # compute tensor product\n",
    "        messages = self.tensor_product(atomic_features[edge_index[0]], sph_harm, edge_z)\n",
    "        # update node features\n",
    "        hidden_feats = (\n",
    "            scatter(messages, edge_index[1], dim=0, dim_size=atomic_features.size(0))\n",
    "            / self.degree_norm\n",
    "        )\n",
    "        return hidden_feats\n",
    "\n",
    "\n",
    "class ScalarReadoutLayer(nn.Module):\n",
    "    def __init__(self, hidden_irreps: o3.Irreps, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_irreps = hidden_irreps\n",
    "        self.output_irreps = o3.Irreps(f\"{output_dim}x0e\")\n",
    "        self.output_layer = o3.Linear(\n",
    "            irreps_in=hidden_irreps, irreps_out=self.output_irreps\n",
    "        )\n",
    "\n",
    "    def forward(self, node_feats: torch.Tensor) -> torch.Tensor:\n",
    "        return self.output_layer(node_feats)\n",
    "\n",
    "\n",
    "class EquiTritonModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_atom_dim: int,\n",
    "        num_layers: int,\n",
    "        output_dim: int,\n",
    "        l_values: int,\n",
    "        edge_dim: int,\n",
    "        hidden_dim: int,\n",
    "        radius_cutoff: float,\n",
    "        degree_norm: float,\n",
    "        edge_kwargs: dict[str, Any] = {},\n",
    "        sph_harm_kwargs: dict[str, Any] = {},\n",
    "        activation: Callable = nn.functional.silu,\n",
    "        num_atoms: int = 100,\n",
    "        skip_connections: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A neural network model designed for processing molecular graphs.\n",
    "\n",
    "        This class implements a hierarchical architecture with multiple interaction blocks,\n",
    "        allowing for efficient and scalable processing of large molecular datasets.\n",
    "\n",
    "        Parameters:\n",
    "            initial_atom_dim (int): The dimensionality of the atomic embeddings.\n",
    "            num_layers (int): The number of convolutional layers to use.\n",
    "            output_dim (int): The dimensionality of the final scalar features.\n",
    "            l_values (int): A list of spherical harmonics order to consider.\n",
    "            edge_dim (int): The dimensionality of the edge features.\n",
    "            hidden_dim (int): The dimensionality of the hidden state in each interaction block.\n",
    "            radius_cutoff (float): The cutoff distance for radial basis functions.\n",
    "            degree_norm (float): The normalization constant for edge features. Typically square root of the average degree.\n",
    "            edge_kwargs (dict[str, Any], optional): Keyword arguments to pass to the InteractionBlock. Defaults to {}.\n",
    "            sph_harm_kwargs (dict[str, Any], optional): Keyword arguments to pass to the InteractionBlock. Defaults to {}.\n",
    "            activation (Callable, optional): The activation function to use in each interaction block. Defaults to nn.functional.silu.\n",
    "            num_atoms (int, optional): The number of atoms in the embedding table (i.e. unique elements). Defaults to 100.\n",
    "            skip_connections (bool, optional): Whether to enable residual connections between layers. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: A tuple containing the graph-level scalar features and the node features.\n",
    "\n",
    "        Examples:\n",
    "            >>> model = EquiTritonModel(...)\n",
    "            >>> graph = PyGGraph(...).to(device=\"cuda\")\n",
    "            >>> graph_z, z = model(graph)\n",
    "\n",
    "        Note: This class uses PyTorch Geometric's Graph data structure and assumes that the input graph has already been processed using a suitable preprocessing step.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.atomic_embedding = AtomEmbedding(num_atoms, initial_atom_dim)\n",
    "        self.initial_layer = InteractionBlock(\n",
    "            initial_atom_dim,\n",
    "            l_values,\n",
    "            edge_dim,\n",
    "            hidden_dim,\n",
    "            radius_cutoff,\n",
    "            degree_norm,\n",
    "            edge_kwargs,\n",
    "            sph_harm_kwargs,\n",
    "            activation,\n",
    "        )\n",
    "        self.conv_layers = nn.ModuleDict()\n",
    "        for layer_index in range(num_layers + 1):\n",
    "            self.conv_layers[f\"conv_{layer_index}\"] = InteractionBlock(\n",
    "                self.initial_layer.output_dim,\n",
    "                l_values,\n",
    "                edge_dim,\n",
    "                hidden_dim,\n",
    "                radius_cutoff,\n",
    "                degree_norm,\n",
    "                edge_kwargs,\n",
    "                sph_harm_kwargs,\n",
    "                activation,\n",
    "            )\n",
    "        self.scalar_readout = ScalarReadoutLayer(\n",
    "            self.initial_layer.output_irreps, output_dim\n",
    "        )\n",
    "        self.skip_connections = skip_connections\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, graph: PyGGraph) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # determine if the graph is batched or not\n",
    "        is_batched = hasattr(graph, \"ptr\")\n",
    "        # get atom embeddings\n",
    "        atom_z = self.atomic_embedding(graph.z)  # [nodes, initial_atom_dim]\n",
    "        # first message passing step\n",
    "        z = self.initial_layer(atom_z, graph.pos, graph.edge_index)\n",
    "        outputs = {}\n",
    "        for layer_name, layer in self.conv_layers.items():\n",
    "            new_z = layer(z, graph.pos, graph.edge_index)\n",
    "            # add residual connections\n",
    "            if self.skip_connections and new_z.shape == z.shape:\n",
    "                new_z += z\n",
    "            z = new_z\n",
    "            outputs[layer_name] = z\n",
    "        # map final output as scalars\n",
    "        z = self.scalar_readout(z)\n",
    "        # latest node features are in z; we generate graph-level scalar features\n",
    "        # by doing a scatter add\n",
    "        if is_batched:\n",
    "            graph_z = scatter(z, graph.batch, dim=0, dim_size=graph.batch_size)\n",
    "        else:\n",
    "            # for a single graph, just sum up the node features\n",
    "            graph_z = z.sum(dim=0, keepdims=True)\n",
    "        return graph_z, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e568b72-3990-4652-8a32-db375c65a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fake_graph(\n",
    "    num_nodes: int,\n",
    "    num_edges: int,\n",
    "    coord_scale: float = 1.0,\n",
    "    max_atomic_number: int = 100,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    coords = torch.rand(num_nodes, 3, device=device) * coord_scale\n",
    "    edge_index = torch.randint(0, high=num_nodes, size=(2, num_edges), device=device)\n",
    "    atomic_numbers = torch.randint(\n",
    "        0, max_atomic_number, size=(num_nodes,), device=device\n",
    "    )\n",
    "    return coords, edge_index, atomic_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8671f0e8-0da8-4250-9d02-fb30ecb977fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_embedder = EdgeEmbedding(num_basis=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4971d48-3574-44d0-b2fb-27306bc3aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords, edge_index, atomic_numbers = make_fake_graph(\n",
    "    16,\n",
    "    12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55f14f94-51ca-4e5c-ad63-ca830404eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords = torch.ones_like(coords, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67f6e836-f4e0-48cf-981d-4419dda0159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_embedder = AtomEmbedding(100, 64).to(\"cuda\")\n",
    "atom_z = atom_embedder(atomic_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46778ea6-92f2-4d7a-9bd8-aa58013cd1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c82e80f-c4bd-43fe-8846-dab203757992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "layer = InteractionBlock(\n",
    "    64, [2, 3, 4, 5], 10, 32, radius_cutoff=6.0, degree_norm=17**0.5\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0913f1e-b000-4a3b-b723-84a92d16425d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "next_layer = InteractionBlock(\n",
    "    layer.output_irreps, [2, 3, 4, 5], 10, 32, radius_cutoff=6.0, degree_norm=17**0.5\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e893b896-f7b9-4b62-95a1-fe7fb45cf2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InteractionBlock(\n",
       "  (tensor_product): FullyConnectedTensorProduct(64x0e x 1x2e+1x3o+1x4e+1x5o -> 32x2e+32x3o+32x4e+32x5o | 8192 paths | 8192 weights)\n",
       "  (edge_basis): EdgeEmbedding()\n",
       "  (spherical_harmonics): SphericalHarmonicEmbedding()\n",
       "  (fc): FullyConnectedNet[10, 32, 8192]\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2785a28-4d2b-4cf0-972d-60df70595580",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = layer(atom_z, coords, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4b80d729-b5e4-4cb5-802d-c27f2af32a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1235,  0.0030,  0.0861,  ...,  0.3042,  0.1641, -0.0114],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0167, -0.0017,  0.0098,  ..., -0.0773, -0.0233,  0.0344],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0578,  0.0226, -0.0362,  ..., -0.0003,  0.0016,  0.0056],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dae3c6e7-8d8f-49bd-9239-8b4b5c6af499",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = next_layer(o, coords, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a322cad7-60c8-4710-9257-2e3f0d4405be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1184])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "599ae7e0-5a54-429d-af9b-749de7ce2434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3229, -0.0011,  0.2210,  ...,  0.3184,  0.1664, -0.0134],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.7950,  0.0826, -0.4642,  ..., -0.1652, -0.0498,  0.0736],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.1217, -0.0898, -0.0128,  ..., -0.0215,  0.0755, -0.0243],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o + p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ad5de-1172-4bdf-83c9-92eda962a398",
   "metadata": {},
   "source": [
    "## Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e448746-66f8-484f-8971-1c89c8f75135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningQM9(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_path: str = \"./qm9_data\",\n",
    "        batch_size: int = 16,\n",
    "        train_frac: float = 0.8,\n",
    "        val_frac: float = 0.1,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom data module for QM9 dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        root_path : str, optional (default: \"./qm9_data\")\n",
    "            Path to the QM9 dataset.\n",
    "        batch_size : int, optional (default: 16)\n",
    "            Number of samples in each mini-batch.\n",
    "        train_frac : float, optional (default: 0.8)\n",
    "            Fraction of data used for training.\n",
    "        val_frac : float, optional (default: 0.1)\n",
    "            Fraction of data used for validation.\n",
    "        num_workers : int, optional (default: 0)\n",
    "            Number of worker processes to use for loading data.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> dm = LightningQM9(root_path=\"/path/to/qm9_data\", batch_size=32)\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        dataset : QM9\n",
    "            Loaded QM9 dataset.\n",
    "        hparams : dict\n",
    "            Hyperparameters of the data module.\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        setup(stage: str)\n",
    "            Setup data splits for training, validation and testing.\n",
    "        train_dataloader()\n",
    "            Returns a DataLoader instance for training data.\n",
    "        val_dataloader()\n",
    "            Returns a DataLoader instance for validation data.\n",
    "        test_dataloader()\n",
    "            Returns a DataLoader instance for testing data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset = QM9(root_path)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        hparams = self.hparams\n",
    "        num_samples = len(self.dataset)\n",
    "        num_train = int(num_samples * hparams[\"train_frac\"])\n",
    "        num_val = int(num_samples * hparams[\"val_frac\"])\n",
    "        num_test = ceil(\n",
    "            num_samples * (1 - (hparams[\"train_frac\"] + hparams[\"val_frac\"]))\n",
    "        )\n",
    "        # generate random splits\n",
    "        train_split, val_split, test_split = random_split(\n",
    "            self.dataset, lengths=[num_train, num_val, num_test]\n",
    "        )\n",
    "        self.splits = {\"train\": train_split, \"val\": val_split, \"test\": test_split}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.splits[\"train\"],\n",
    "            batch_size=self.hparams[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=self.hparams[\"num_workers\"],\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.splits[\"val\"],\n",
    "            batch_size=self.hparams[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams[\"num_workers\"],\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.splits[\"test\"],\n",
    "            batch_size=self.hparams[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=self.hparams[\"num_workers\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c15f669-04ca-418b-8f22-81993eafbce0",
   "metadata": {},
   "source": [
    "## Loss and Lightning module\n",
    "\n",
    "Model trains optionally with a loss target that Nequip and MACE uses, which is the atom-weighted MSE. For now we're only using a single target, but can expand to use the full QM9 set of targets too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6affd332-fe3a-49dd-b0c2-e20fcb974a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomWeightedMSE(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the mean-squared-error between predicted and targets,\n",
    "    weighted by the number of atoms within each graph.\n",
    "\n",
    "    From matsciml\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        atoms_per_graph: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        if atoms_per_graph.size(0) != target.size(0):\n",
    "            raise RuntimeError(\n",
    "                \"Dimensions for atom-weighted loss do not match:\"\n",
    "                f\" expected atoms_per_graph to have {target.size(0)} elements; got {atoms_per_graph.size(0)}.\"\n",
    "                \"This loss is intended to be applied to scalar targets only.\"\n",
    "            )\n",
    "        # check to make sure we are broad casting correctly\n",
    "        if (input.ndim != target.ndim) and target.size(-1) == 1:\n",
    "            input.unsqueeze_(-1)\n",
    "        # for N-d targets, we might want to keep unsqueezing\n",
    "        while atoms_per_graph.ndim < target.ndim:\n",
    "            atoms_per_graph.unsqueeze_(-1)\n",
    "        # ensures that atoms_per_graph is type cast correctly\n",
    "        squared_error = ((input - target) / atoms_per_graph.to(input.dtype)) ** 2.0\n",
    "        return squared_error.mean()\n",
    "\n",
    "\n",
    "class EquiTritonLitModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class: type,\n",
    "        model_kwargs,\n",
    "        e_mean: float,\n",
    "        e_std: float,\n",
    "        lr: float = 1e-3,\n",
    "        weight_decay: float = 0.0,\n",
    "        atom_weighted_loss: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the EquiTritonLitModule clas.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model_class : type\n",
    "            Th class of the model to be used.\n",
    "        model_kwargs : dict\n",
    "            Keyword argument for the model initialization.\n",
    "        e_mean : float\n",
    "            The mean of the energy values.\n",
    "        e_std : float\n",
    "            The standard deviation of the energy values.\n",
    "        lr : float, optional\n",
    "            The learning rate (default is 1e-3) for AdamW.\n",
    "        weight_decay : float, optional\n",
    "            Weight decay value (default is 0.0).\n",
    "        atom_weighted_loss : bool, optional\n",
    "            Whether to use atom-weighted loss or not (default is True).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model_class(**model_kwargs)\n",
    "        if atom_weighted_loss:\n",
    "            self.loss = AtomWeightedMSE()\n",
    "        else:\n",
    "            self.loss = nn.MSELoss()\n",
    "        self.output_head = nn.Linear(self.model.output_dim, 1)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams[\"lr\"],\n",
    "            weight_decay=self.hparams[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "    def step(self, graph: PyGGraph, stage: Literal[\"train\", \"test\", \"val\"]):\n",
    "        \"\"\"\n",
    "        Performs a single step of the training, validation or testing\n",
    "        process.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        graph : PyGGraph\n",
    "            The input graph.\n",
    "        stage : Literal[\"train\", \"test\", \"val\"]\n",
    "            The current stage (training, testing or validation).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            The calculated loss value.\n",
    "        \"\"\"\n",
    "        g_z, z = self.model(graph)\n",
    "        pred_energy = self.output_head(g_z)\n",
    "        target_energy = graph.y[:, 12].unsqueeze(-1)\n",
    "        norm_energy = (target_energy - self.hparams[\"e_mean\"]) / self.hparams[\"e_std\"]\n",
    "        if self.hparams[\"atom_weighted_loss\"]:\n",
    "            loss = self.loss(pred_energy, norm_energy, torch.diff(graph.ptr))\n",
    "        else:\n",
    "            loss = self.loss(pred_energy, norm_energy)\n",
    "        batch_size = getattr(graph, \"batch_size\", 1)\n",
    "        self.log(\n",
    "            f\"{stage}_loss\", loss, prog_bar=True, on_step=True, batch_size=batch_size\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss = self.step(batch, \"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        loss = self.step(batch, \"val\")\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        loss = self.step(batch, \"test\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d7ce968-f33c-46dd-88e3-8ad47480a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch_geometric/io/fs.py:215: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location)\n"
     ]
    }
   ],
   "source": [
    "dm = LightningQM9(\"./qm9_data/\", batch_size=64)\n",
    "dm.setup(\"fit\")\n",
    "\n",
    "train_loader = dm.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c89a710-1e68-485c-9325-0b06c7b52b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.cat([sample.y[:, 12] for sample in dm.dataset])\n",
    "e_mean = values.mean()\n",
    "e_std = values.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef3c9902-3673-4c88-a98e-1a4553a5990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lit_mod = EquiTritonLitModule(\n",
    "    EquiTritonModel,\n",
    "    model_kwargs={\n",
    "        \"initial_atom_dim\": 64,\n",
    "        \"num_layers\": 3,\n",
    "        \"output_dim\": 48,\n",
    "        \"l_values\": [0, 1, 2, 4, 6, 8],\n",
    "        \"edge_dim\": 10,\n",
    "        \"hidden_dim\": 16,\n",
    "        \"radius_cutoff\": 6.0,\n",
    "        \"degree_norm\": 37.5**0.5,\n",
    "    },\n",
    "    e_mean=e_mean,\n",
    "    e_std=e_std,\n",
    "    atom_weighted_loss=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1104fe2c-ada6-448b-9e3d-cab1bc491c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=30, accelerator=\"gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e30a0671-efa3-403b-9d8b-cc2cb63bff20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type            | Params\n",
      "------------------------------------------------\n",
      "0 | model       | EquiTritonModel | 4.8 M \n",
      "1 | loss        | MSELoss         | 0     \n",
      "2 | output_head | Linear          | 49    \n",
      "------------------------------------------------\n",
      "4.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.8 M     Total params\n",
      "19.300    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n",
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d990cd3f7cca46fe8dac82edee407d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelvin/miniforge3/envs/equitriton/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(lit_mod, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663a128-606c-4944-8877-73deafc30305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
